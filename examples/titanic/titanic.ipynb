{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Dataset\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Include all libraries\n",
    "\n",
    "Include all required libraries and output the version information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "np.__version__, pd.__version__, sklearn.__version__, sns.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext version_information\n",
    "%version_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please manually download the dataset `train.csv` from [Kaggle](https://www.kaggle.com/competitions/titanic/data) (after sign-in) into the folder `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_csv_path = 'data/train.csv'\n",
    "\n",
    "while not os.path.isfile(raw_train_csv_path):\n",
    "    print('Please sign in to Kaggle and download the dataset `train.csv` into the folder `data`.')\n",
    "    input('Press Enter to continue...')\n",
    "\n",
    "raw_df = pd.read_csv(raw_train_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first 5 records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shape of the records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In which columns are how many `null` values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in raw_df:\n",
    "    if (raw_df[column].isnull().sum()):\n",
    "        print(f'{column}: {raw_df[column].isnull().sum()} null values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_raw_df(df):\n",
    "    # Remove unneeded columns \n",
    "    df = df.drop(['Name', 'Ticket'], axis=1)\n",
    "    # Remove columns with missing values\n",
    "    df = df.drop(['Cabin'], axis=1)\n",
    "    # Remove the two records with missing `Embarked` values\n",
    "    df = df.dropna(axis=0, subset=['Embarked'])\n",
    "    # Calculate missing `Age` values\n",
    "    df['Age'] = df.groupby(['Pclass','Sex'])['Age'].transform(lambda x: x.fillna(x.median()))\n",
    "    # Replace categorical values with dummy values\n",
    "    df['Sex'] = df['Sex'].replace({'male': 0, 'female': 1})\n",
    "    df['Embarked'] = df['Embarked'].replace({'S': 0, 'C': 1, 'Q': 2})\n",
    "    # Reorder columns\n",
    "    df = df[['PassengerId', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Survived']]\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={ 'PassengerId': 'id', 'Pclass': 'ticket_class', 'Sex': 'sex', 'Age': 'age',\n",
    "                             'SibSp': 'num_of_siblings_and_spouses', 'Parch': 'num_of_parents_and_children', \n",
    "                             'Fare': 'fare', 'Embarked': 'port_of_embarkation', 'Survived': 'survived'})\n",
    "    return df\n",
    "\n",
    "df = prepare_raw_df(raw_df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the distribution of data for column `survived`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column=\"survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the X and y values and split the data set into training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(['id', 'survived'], axis=1)\n",
    "y = df['survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit and validate the model and plot the ROC curve and confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, plot_confusion_matrix\n",
    "\n",
    "def plot_roc_curve(false_positive_rate, true_positive_rate, roc_auc):\n",
    "    lw = 2\n",
    "    plt.plot(false_positive_rate, true_positive_rate, color='darkorange', lw=lw, label=\"ROC curve (area = %0.2f)\" % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def validate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy = {}'.format(accuracy))\n",
    "\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    print('Precision = {}'.format(precision))\n",
    "\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    print('Recall = {}'.format(recall))\n",
    "\n",
    "    f1Score = f1_score(y_test, y_pred)\n",
    "    print('F1-Score = {}'.format(f1Score))\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, y_pred) \n",
    "    y_prob = model.predict_proba(X_test)  \n",
    "    y_prob = y_prob[:, 1]  \n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob) \n",
    "    plot_roc_curve(false_positive_rate, true_positive_rate, roc_auc)\n",
    "\n",
    "    plot_confusion_matrix(model, X_test, y_test, cmap=plt.cm.Blues, normalize = 'all')\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "validate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Website](https://superwise.ai/) | [Documentation](https://docs.superwise.ai/docs) | [Portal](https://portal.superwise.ai/)\n",
    "\n",
    "#### Setup\n",
    "\n",
    "Create a new secret in the Superwise portal and enter the client id and the secret:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superwise import Superwise\n",
    "\n",
    "print(\"Enter your Superwise Client ID...\")\n",
    "os.environ['SUPERWISE_CLIENT_ID'] = input()\n",
    "\n",
    "print(\"Enter your Superwise Secret...\")\n",
    "os.environ['SUPERWISE_SECRET'] = getpass.getpass()\n",
    "\n",
    "sw_client = Superwise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new project and a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superwise.models.project import Project\n",
    "from superwise.models.model import Model\n",
    "\n",
    "# Create a new project\n",
    "project = Project(name=f'Titanic Project {datetime.today().strftime(\"%Y-%m-%d\")}', description='Project for the Titanic model.')\n",
    "project = sw_client.project.create(project)\n",
    "sw_project_id = project.id\n",
    "print('Superwise Project ID:', sw_project_id)\n",
    "\n",
    "# Create a new model\n",
    "sw_model = Model(name=f'Titanic Model  {datetime.today().strftime(\"%Y-%m-%d\")}', description='The Titanic model.', project_id=sw_project_id)\n",
    "sw_model = sw_client.model.create(sw_model)\n",
    "sw_model_id = sw_model.id\n",
    "print('Superwise Model ID:', sw_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sw_client.model.delete_by_id(sw_model_id)\n",
    "# sw_client.project.delete_by_id(sw_project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look: https://portal.superwise.ai/\n",
    "\n",
    "Next, we have to upload our dataset.\n",
    "\n",
    "> Uploading a dataset is an essential step when you create a version in the Superwise platform. It acts as a model baseline and is used by Superwise as reference data when carrying out analytics and pre-configured metrics.\n",
    "\n",
    "Prepare the training data for the upload to Superwise and create a dataset and version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from superwise.models.dataset import Dataset\n",
    "from superwise.models.version import Version\n",
    "from superwise.resources.superwise_enums import DataEntityRole, FeatureType, DatasetType\n",
    "\n",
    "def sw_prepare_train_data(df):\n",
    "    df_copy = df.copy(deep=True)\n",
    "    df_copy['prediction'] = model.predict(df.drop(['id', 'survived'], axis=1))\n",
    "    df_copy['prediction_probability'] = np.max(model.predict_proba(X), axis=1)\n",
    "    df_copy['timestamp'] = datetime.today().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    return df_copy\n",
    "\n",
    "sw_train_csv_path = 'data/sw_train.csv'\n",
    "\n",
    "sw_df = sw_prepare_train_data(df)\n",
    "sw_df.to_csv(sw_train_csv_path, index=False)\n",
    "\n",
    "# Create a new dataset\n",
    "sw_dataset = Dataset(name=f'Training Dataset', \n",
    "                     files=[sw_train_csv_path], \n",
    "                     project_id=sw_project_id,\n",
    "                     type=DatasetType.TRAIN.value,\n",
    "                     roles={\n",
    "                        DataEntityRole.LABEL.value: ['survived'],\n",
    "                        DataEntityRole.PREDICTION_VALUE.value: ['prediction', 'prediction_probability'],\n",
    "                        DataEntityRole.TIMESTAMP.value: 'timestamp',\n",
    "                        DataEntityRole.ID.value: 'id'\n",
    "                     },\n",
    "                     dtypes={\n",
    "                        'sex': FeatureType.CATEGORICAL.value,\n",
    "                        'prediction': FeatureType.BOOLEAN.value,\n",
    "                        'survived': FeatureType.BOOLEAN.value\n",
    "                    })\n",
    "sw_dataset = sw_client.dataset.create(sw_dataset)\n",
    "sw_dataset_id = sw_dataset.id\n",
    "print('Superwise Dataset ID:', sw_dataset_id)\n",
    "\n",
    "# Create a new version\n",
    "sw_version = Version(name=datetime.today().strftime('%Y-%m-%d-%H-%M-%S'), model_id=sw_model_id, dataset_id=sw_dataset_id)\n",
    "sw_version = sw_client.version.create(sw_version)\n",
    "sw_version_id = sw_version.id\n",
    "print('Superwise Version ID:', sw_version_id)\n",
    "\n",
    "# Upload the new version\n",
    "sw_version = sw_client.version.activate(sw_version_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look: https://portal.superwise.ai/\n",
    "\n",
    "Create two segments for the genders in the portal.\n",
    "\n",
    "Make a prediction and log the result in Superwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sw_log_prediction(input_dict: dict, prediction: int, prediction_probability: float):\n",
    "    record = {i[0]: i[1] for i in input_dict.items()}\n",
    "    record['id'] = datetime.today().strftime('%Y%m%d%H%M%S%f')\n",
    "    record['survived'] = None\n",
    "    record['prediction'] = prediction\n",
    "    record['prediction_probability'] = prediction_probability\n",
    "    record['timestamp'] = datetime.today().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    sw_client.transaction.log_records(\n",
    "        model_id = sw_model_id, \n",
    "        version_id = sw_version_id, \n",
    "        records = [record]\n",
    "    )['transaction_id']\n",
    "\n",
    "def predict(model, input_dict: dict):\n",
    "    input_df = pd.DataFrame(data={i[0]: [i[1]] for i in input_dict.items()})\n",
    "    return int(model.predict(input_df)[0]), float(np.max(model.predict_proba(input_df), axis=1)[0])\n",
    "\n",
    "prediction_input = {\n",
    "    'ticket_class': 2,\n",
    "    'sex': 1,\n",
    "    'age': 20,\n",
    "    'num_of_siblings_and_spouses': 0,\n",
    "    'num_of_parents_and_children': 0,\n",
    "    'fare': 10,\n",
    "    'port_of_embarkation': 1\n",
    "}\n",
    "prediction, prediction_probability = predict(model, prediction_input)\n",
    "print(prediction, prediction_probability)\n",
    "sw_log_prediction(prediction_input, prediction, prediction_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send requests to the web server and submit the input and predictions directly to Superwise:\n",
    "\n",
    "**Task**: Can you figure out which parameters are most relevant based on various inputs and predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask_restx import Api, Resource, reqparse\n",
    "\n",
    "flask_app = Flask(__name__)\n",
    "\n",
    "flask_api = Api(flask_app)\n",
    "\n",
    "flask_request_parser = reqparse.RequestParser()\n",
    "flask_request_parser.add_argument('ticket_class', type=int, help='Ticket class: 1 = 1st (Upper), 2 = 2nd (Middle), 3 = 3rd (Lower)', required=True)\n",
    "flask_request_parser.add_argument('sex', type=int, help='Sex: 0 = Man, 1 = Woman', required=True)\n",
    "flask_request_parser.add_argument('age', type=int, help='Age in years', required=True)\n",
    "flask_request_parser.add_argument('num_of_siblings_and_spouses', type=int, help='Number of siblings and spouses aboard the Titanic', required=True)\n",
    "flask_request_parser.add_argument('num_of_parents_and_children', type=int, help='Number of parents and children aboard the Titanic', required=True)\n",
    "flask_request_parser.add_argument('fare', type=float, help='Passenger fare', required=True)\n",
    "flask_request_parser.add_argument('port_of_embarkation', type=int, help='Port of embarkation: 0 = Southampton, 1 = Cherbourg, 2 = Queenstown', required=True)\n",
    "\n",
    "@flask_api.route('/predict')\n",
    "class PredictionResource(Resource):\n",
    "    @flask_api.doc(parser=flask_request_parser)\n",
    "    def post(self):\n",
    "        args = flask_request_parser.parse_args()\n",
    "        # Make prediction\n",
    "        prediction, prediction_probability = predict(model, args)\n",
    "        print()\n",
    "        print('> Input:', args)\n",
    "        print('> Prediction:', prediction)\n",
    "        print('> Probability:', prediction_probability)\n",
    "        print()\n",
    "        # Log production predictions\n",
    "        sw_log_prediction(args, prediction, prediction_probability)\n",
    "        return { 'prediction': prediction, 'prediction_probability': prediction_probability }\n",
    "\n",
    "flask_app.run(port=5100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the requests on the Superwise Analytics page and filter by the segments.\n",
    "\n",
    "Also, look at the available metrics and create policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of correlations between `survived` and the rest of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corrwith(df['survived']).drop(['survived'])\n",
    "corr_df = pd.DataFrame({ 'survived': corr }, index=df.columns[:-1])\n",
    "cmap = sns.diverging_palette(10,10, as_cmap=True)\n",
    "sns.heatmap(corr_df, annot=True, cmap=cmap, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "### WhyLabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your org-id here - should be something like \"org-xxxx\"\n",
    "print(\"Enter your WhyLabs Org ID...\") \n",
    "os.environ[\"WHYLABS_DEFAULT_ORG_ID\"] = input()\n",
    "\n",
    "# Set your datased_id (or model_id) here - should be something like \"model-xxxx\"\n",
    "print(\"Enter your WhyLabs Dataset ID...\")\n",
    "os.environ[\"WHYLABS_DEFAULT_DATASET_ID\"] = input()\n",
    "\n",
    "# Set your API key here\n",
    "print(\"Enter your WhyLabs API key...\")\n",
    "os.environ[\"WHYLABS_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into daily batches and upload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timezone, timedelta\n",
    "import whylogs as why\n",
    "from whylogs.api.writer.whylabs import WhyLabsWriter\n",
    "\n",
    "wl_writer = WhyLabsWriter()\n",
    "\n",
    "def wl_prepare_train_data(df):\n",
    "    df_copy = df.copy(deep=True)\n",
    "    df_copy.rename(columns={ 'survived': 'output_survived' }, inplace=True)\n",
    "    df_copy['output_prediction'] = model.predict(df.drop(['id', 'survived'], axis=1))\n",
    "    df_copy['output_score'] = np.max(model.predict_proba(X), axis=1)\n",
    "    return df_copy\n",
    "\n",
    "def wl_log_data(df, timestamp):\n",
    "    # wl_results = why.log_classification_metrics(\n",
    "    #     df,\n",
    "    #     target_column = 'output_survived',\n",
    "    #     prediction_column = 'output_prediction',\n",
    "    #     score_column='output_score'\n",
    "    # )\n",
    "    # wl_profile = wl_results.profile()\n",
    "    # wl_profile.set_dataset_timestamp(timestamp)\n",
    "    # wl_results.writer(\"whylabs\").write()\n",
    "    wl_profile = why.log(df).profile()\n",
    "    wl_profile.set_dataset_timestamp(timestamp)\n",
    "    wl_writer.write(profile=wl_profile.view())\n",
    "    print('Logged profile for {}'.format(timestamp))\n",
    "\n",
    "def add_random_column_outliers(df, column, num_of_outliers):\n",
    "    df = df.copy(deep=True)\n",
    "    df = df.reset_index()\n",
    "    number_of_rows = df.shape[0]\n",
    "    for i in range(num_of_outliers):\n",
    "        random_row = np.random.randint(0, number_of_rows)\n",
    "        df.loc[random_row, column] += round(np.random.uniform(low=150.0, high=200.0), 2)\n",
    "    return df\n",
    "\n",
    "wl_df = wl_prepare_train_data(df)\n",
    "wl_df_splits = np.array_split(wl_df, 7)\n",
    "\n",
    "for day, df_split in enumerate(wl_df_splits):\n",
    "    if day == 2:\n",
    "        # df_split = add_random_column_outliers(df_split, 'age', 30)\n",
    "        df_split['age'] += 50\n",
    "    timestamp = datetime.now(timezone.utc) - timedelta(days=day + 1)\n",
    "    wl_log_data(df_split, timestamp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
